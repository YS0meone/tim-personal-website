# July 3, 2025 - Month Plan setup

**Duration**: 3 hours  
**Focus**: Deep Learning Basics  
**Difficulty**: â­â­â­â­â˜†

## ğŸ¯ Daily Goals

- [x] Create the montly goal
- [ ] First Chapter of AI Engineering Book

## ğŸ“ What I Learned

### Backpropagation Algorithm

- **Chain rule application**: How gradients flow backward through the network
- **Computational graph**: Visual representation of forward and backward passes
- **Key insight**: Each layer's gradients depend on the next layer's gradients

```python
def backprop(self, x, y):
    # Forward pass
    activations = [x]
    zs = []

    for b, w in zip(self.biases, self.weights):
        z = np.dot(w, activations[-1]) + b
        zs.append(z)
        activations.append(sigmoid(z))

    # Backward pass
    delta = self.cost_derivative(activations[-1], y) * sigmoid_prime(zs[-1])
    # ... rest of implementation
```

### Gradient Descent Variants

- **SGD**: Stochastic gradient descent - updates after each sample
- **Mini-batch**: Balance between efficiency and accuracy
- **Momentum**: Helps overcome local minima

## ğŸ”— Resources Used

- [Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/) - Chapter 2
- [3Blue1Brown Neural Networks Series](https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi)
- [Backpropagation Calculus](https://www.youtube.com/watch?v=tIeHLnjs5U8)

## ğŸ’¡ Key Insights

1. **Gradient descent is like finding the steepest path down a mountain**
2. **Learning rate is crucial** - too high and you overshoot, too low and you're too slow
3. **Initialization matters** - Xavier/He initialization prevents vanishing gradients

## ğŸš€ Next Steps

- Implement different activation functions (ReLU, Leaky ReLU)
- Study regularization techniques (dropout, L1/L2)
- Build a CNN for image classification

## ğŸ¤” Questions & Challenges

- Why does ReLU work better than sigmoid for deep networks?
- How to choose optimal learning rate automatically?
- What's the intuition behind batch normalization?

